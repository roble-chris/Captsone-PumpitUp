{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modelling : Multiclassification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/galopito/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Import libraries\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, average_precision_score\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, roc_curve, make_scorer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_curve, confusion_matrix,precision_recall_curve,confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, fbeta_score, roc_auc_score\n",
    "from sklearn.metrics import plot_precision_recall_curve, plot_confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Modeling\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict,GridSearchCV,RandomizedSearchCV, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['precision_weighted']\n",
    "RSEED = 42\n",
    "DAT = './data/'\n",
    "PARAMS = './data/'\n",
    "pd.set_option('display.max_columns', 50)\n",
    "random_state = 100\n",
    "test_size = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of Dataset and Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of dataset after cleaning\n",
    "df = pd.read_csv(DAT + 'df_model.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Converting Labels  \n",
    "target_status_group = {'functional':2, 'functional needs repair': 1, 'non functional' : 0}\n",
    "df['status_group'] = df['status_group'].replace(target_status_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    32259\n",
       "0    22824\n",
       "1     4317\n",
       "Name: status_group, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['status_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictors and target variable\n",
    "X = df.drop([\"status_group\"], axis=1)\n",
    "y = df[\"status_group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41580, 22)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59400 entries, 0 to 59399\n",
      "Data columns (total 23 columns):\n",
      "amount_tsh           59400 non-null float64\n",
      "gps_height           59400 non-null float64\n",
      "longitude            59400 non-null float64\n",
      "latitude             59400 non-null float64\n",
      "basin                59400 non-null object\n",
      "region               59400 non-null object\n",
      "lga                  59400 non-null object\n",
      "public_meeting       59400 non-null object\n",
      "recorded_by          59400 non-null object\n",
      "scheme_management    59400 non-null object\n",
      "permit               59400 non-null object\n",
      "construction_year    59400 non-null int64\n",
      "payment_type         59400 non-null object\n",
      "quality_group        59400 non-null object\n",
      "quantity             59400 non-null object\n",
      "source               59400 non-null object\n",
      "waterpoint_type      59400 non-null object\n",
      "status_group         59400 non-null int64\n",
      "funder_cat           59400 non-null object\n",
      "installer_cat        59400 non-null object\n",
      "extraction_custom    59400 non-null object\n",
      "management_custom    59400 non-null object\n",
      "population_log       59400 non-null float64\n",
      "dtypes: float64(5), int64(2), object(16)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basin',\n",
       " 'region',\n",
       " 'lga',\n",
       " 'public_meeting',\n",
       " 'recorded_by',\n",
       " 'scheme_management',\n",
       " 'permit',\n",
       " 'payment_type',\n",
       " 'quality_group',\n",
       " 'quantity',\n",
       " 'source',\n",
       " 'waterpoint_type',\n",
       " 'funder_cat',\n",
       " 'installer_cat',\n",
       " 'extraction_custom',\n",
       " 'management_custom']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating list for categorical predictors/features (used in \"Scaling with Preprocessing Pipeline\") \n",
    "cat_features = list(df.columns[df.dtypes==object])\n",
    "#cat_features.remove('status_group')\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_features.remove('permit')\n",
    "#cat_features.remove('public_meeting')\n",
    "#cat_features.remove('lga')\n",
    "#cat_features.remove('ward')\n",
    "#cat_features.remove('recorded_by')\n",
    "#cat_features.remove('region_code')\n",
    "#num_features.remove('num_private')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basin',\n",
       " 'region',\n",
       " 'lga',\n",
       " 'public_meeting',\n",
       " 'recorded_by',\n",
       " 'scheme_management',\n",
       " 'permit',\n",
       " 'payment_type',\n",
       " 'quality_group',\n",
       " 'quantity',\n",
       " 'source',\n",
       " 'waterpoint_type',\n",
       " 'funder_cat',\n",
       " 'installer_cat',\n",
       " 'extraction_custom',\n",
       " 'management_custom']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount_tsh',\n",
       " 'gps_height',\n",
       " 'longitude',\n",
       " 'latitude',\n",
       " 'construction_year',\n",
       " 'population_log']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating list for numerical predictors/features (removing target column, used in \"Scaling with Preprocessing Pipeline\")\n",
    "num_features = list(df.columns[df.dtypes!=object])\n",
    "num_features.remove('status_group')\n",
    "#num_features.remove('district_code')\n",
    "#num_features.remove('population')\n",
    "#num_features.remove('region_code')\n",
    "num_features\n",
    "#cat_features.remove('status_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline using Pipeline\n",
    "# Pipeline for numerical features\n",
    "num_pipeline = Pipeline([\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features \n",
    "cat_pipeline = Pipeline([\n",
    "    ('1hot', OneHotEncoder(sparse=False))\n",
    "])\n",
    "\n",
    "# Complete pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (41580, 293)\n",
      "validation shape:  (17820, 293)\n"
     ]
    }
   ],
   "source": [
    "print('train shape: ', X_train_transformed.shape)\n",
    "print('validation shape: ',X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5430976430976431"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DummyClassifier for multiclass target variable \n",
    "#(2 = 'functional', 1 = 'functional needs repair', 0 = 'non functional')\n",
    "\n",
    "dummy = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "dummy.fit(X_train_transformed, y_train)\n",
    "dummy.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 54.31%\n",
      "Mean precision: 29.49%\n",
      "Mean recall: 54.31%\n"
     ]
    }
   ],
   "source": [
    "metrics = ['accuracy', 'precision_weighted', 'recall_weighted']\n",
    "scores_dummy = cross_validate(dummy, X_train_transformed, y_train, scoring=metrics, cv=5, n_jobs=-1)\n",
    "print('Mean accuracy: {:.2f}%'.format(scores_dummy['test_accuracy'].mean()*100))\n",
    "print('Mean precision: {:.2f}%'.format(scores_dummy['test_precision_weighted'].mean()*100))\n",
    "print('Mean recall: {:.2f}%'.format(scores_dummy['test_recall_weighted'].mean()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression (using pipeline)\n",
    "pipeline_logreg = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(pipeline_logreg,X_train,y_train,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores:\n",
      "-------------------------\n",
      "Accuracy: 0.75\n",
      "Recall: 0.57\n",
      "Precision: 0.69\n",
      "F1 Score: 0.59\n",
      "Confusion Matrix: \n",
      "[[10699   159  5119]\n",
      " [  584   447  1991]\n",
      " [ 2267   216 20098]]\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression (using pipeline, printing results)\n",
    "print('Cross validation scores:')\n",
    "print('-------------------------')\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_train, y_train_pred)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_train, y_train_pred,average='macro')))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_train, y_train_pred,average='macro')))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_train, y_train_pred,average='macro')))\n",
    "print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y_train, y_train_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters for grid-search (C initial: [0.01, 0.1, 1, 10, 100]; adapted according to optimal results)\n",
    "param_logreg = {'logreg__penalty':('l1','l2'),\n",
    "                'logreg__C': [0.05, 0.08, 0.1, 0.2, 0.5, 1],\n",
    "                'logreg__class_weight': [{0: x, 1: 1.0-x} for x in [0.25,0.5,0.75]]\n",
    "               }\n",
    "\n",
    "grid_logreg = GridSearchCV(pipeline_logreg, param_grid=param_logreg, cv=5, scoring=scoring, \n",
    "                           verbose=5, n_jobs=-1, refit='precision_weighted',return_train_score=True) # scoring can also be \"precision\", \"recall\", ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:  7.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocessor',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('num',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('std_scaler',\n",
       "                                                                                          StandardScaler(copy=True,\n",
       "                                                                                                         with_mean=True,\n",
       "                                                                                                         with_std=True))],\n",
       "                                                                                  verbose=False),\n",
       "                                                                         ['amount_tsh',\n",
       "                                                                          'gps_height',\n",
       "                                                                          'longit...\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'logreg__C': [0.05, 0.08, 0.1, 0.2, 0.5, 1],\n",
       "                         'logreg__class_weight': [{0: 0.25, 1: 0.75},\n",
       "                                                  {0: 0.5, 1: 0.5},\n",
       "                                                  {0: 0.75, 1: 0.25}],\n",
       "                         'logreg__penalty': ('l1', 'l2')},\n",
       "             pre_dispatch='2*n_jobs', refit='precision_weighted',\n",
       "             return_train_score=True, scoring=['precision_weighted'],\n",
       "             verbose=5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "grid_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.77\n",
      "Best parameters:\n",
      "{'logreg__C': 0.2, 'logreg__class_weight': {0: 0.75, 1: 0.25}, 'logreg__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Show best parameters\n",
    "print('Best score:\\n{:.2f}'.format(grid_logreg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_logreg.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model as best_model\n",
    "best_model_logreg = grid_logreg.best_estimator_['logreg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Recall: 0.52\n",
      "Precision: 0.74\n",
      "F1 Score: 0.51\n",
      "Confusion Matrix: \n",
      "[[4270    1 2576]\n",
      " [ 199    4 1092]\n",
      " [ 660    1 9017]]\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy, recall and precision for the test set with the optimized model\n",
    "y_pred_logreg = best_model_logreg.predict(X_test_transformed)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred_logreg)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_test, y_pred_logreg,average='macro')))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_test, y_pred_logreg,average='macro')))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_test, y_pred_logreg,average='macro')))\n",
    "print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y_test, y_pred_logreg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Support Vector Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "pipeline_rf_clf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('rf_clf', RandomForestClassifier(n_estimators=100,\n",
    "                              random_state=random_state,\n",
    "                              max_depth=5,\n",
    "                              max_features=\"sqrt\",\n",
    "                              n_jobs=-1,\n",
    "                              class_weight='balanced',\n",
    "                              criterion= 'entropy',\n",
    "                              min_samples_split= 10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_rf_clf = cross_val_predict(pipeline_rf_clf, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores:\n",
      "-------------------------\n",
      "Accuracy: 0.64\n",
      "Recall: 0.61\n",
      "Precision: 0.57\n",
      "F1 Score: 0.56\n",
      "Confusion Matrix: \n",
      "[[ 9705  2131  4141]\n",
      " [  458  1703   861]\n",
      " [ 2664  4814 15103]]\n"
     ]
    }
   ],
   "source": [
    "print('Cross validation scores:')\n",
    "print('-------------------------')\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_train, y_train_pred_rf_clf)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_train, y_train_pred_rf_clf,average='macro')))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_train, y_train_pred_rf_clf,average='macro')))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_train, y_train_pred_rf_clf,average='macro')))\n",
    "print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y_train, y_train_pred_rf_clf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameter space for grid-search\n",
    "param_grid = {'rf_clf__bootstrap': [True, False],\n",
    "              'rf_clf__max_depth': [2, 3, 5, 10, 20, 30,50,None],\n",
    "              'rf_clf__max_features': ['auto', 'sqrt',3,5,10,20],\n",
    "              'rf_clf__min_samples_leaf': [1, 2, 4],\n",
    "              'rf_clf__min_samples_split': [2, 5, 10],\n",
    "              'rf_clf__n_estimators': [10, 50, 100, 200, 400]} # Others: kernel, degree (only for poly)\n",
    "grid_rf_clf = RandomizedSearchCV(pipeline_rf_clf, param_grid, cv=5, scoring=scoring, \n",
    "                           verbose=10, n_jobs=-1, refit='precision_weighted',return_train_score=True, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   33.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 18.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('preprocessor',\n",
       "                                              ColumnTransformer(n_jobs=None,\n",
       "                                                                remainder='drop',\n",
       "                                                                sparse_threshold=0.3,\n",
       "                                                                transformer_weights=None,\n",
       "                                                                transformers=[('num',\n",
       "                                                                               Pipeline(memory=None,\n",
       "                                                                                        steps=[('std_scaler',\n",
       "                                                                                                StandardScaler(copy=True,\n",
       "                                                                                                               with_mean=True,\n",
       "                                                                                                               with_std=True))],\n",
       "                                                                                        verbose=False),\n",
       "                                                                               ['amount_tsh',\n",
       "                                                                                'gps_height',\n",
       "                                                                                '...\n",
       "                   param_distributions={'rf_clf__bootstrap': [True, False],\n",
       "                                        'rf_clf__max_depth': [2, 3, 5, 10, 20,\n",
       "                                                              30, 50, None],\n",
       "                                        'rf_clf__max_features': ['auto', 'sqrt',\n",
       "                                                                 3, 5, 10, 20],\n",
       "                                        'rf_clf__min_samples_leaf': [1, 2, 4],\n",
       "                                        'rf_clf__min_samples_split': [2, 5, 10],\n",
       "                                        'rf_clf__n_estimators': [10, 50, 100,\n",
       "                                                                 200, 400]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None,\n",
       "                   refit='precision_weighted', return_train_score=True,\n",
       "                   scoring=['precision_weighted'], verbose=10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit on training data\n",
    "grid_rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.80\n",
      "Best parameters:\n",
      "{'rf_clf__n_estimators': 400, 'rf_clf__min_samples_split': 5, 'rf_clf__min_samples_leaf': 1, 'rf_clf__max_features': 'auto', 'rf_clf__max_depth': 30, 'rf_clf__bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "# Show best parameters\n",
    "print('Best score:\\n{:.2f}'.format(grid_rf_clf.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_rf_clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model as best_model\n",
    "best_model_rf_clf = grid_rf_clf.best_estimator_['rf_clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n",
      "Recall: 0.71\n",
      "Precision: 0.70\n",
      "F1 Score: 0.70\n",
      "Confusion Matrix: \n",
      "[[5339  235 1273]\n",
      " [ 184  641  470]\n",
      " [ 874  578 8226]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_rf_clf = best_model_rf_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred_rf_clf)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_test, y_pred_rf_clf,average='macro')))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_test, y_pred_rf_clf,average='macro')))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_test, y_pred_rf_clf,average='macro')))\n",
    "print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y_test, y_pred_rf_clf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('logreg', LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pipeline to use in RandomSearchCV and GridSearchCV\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb_reg', XGBClassifier(n_estimators=110,\n",
    "                              random_state=random_state,\n",
    "                              max_depth=5,\n",
    "                              max_features=20,\n",
    "                              scoring=scoring,\n",
    "                              n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Select models for comparison\n",
    "models={#'Baseline': DummyClassifier(strategy='most_frequent'),\n",
    "        'LogReg': LogisticRegression(max_iter=1000),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'SVC': SVC(kernel='rbf', C=1E6),\n",
    "        'Decision Tree': DecisionTreeClassifier(criterion=\"gini\", max_depth=3,random_state=random_state),\n",
    "        'Random Forest': RandomForestClassifier(random_state=random_state, max_features='sqrt', n_jobs=-1),\n",
    "        'Gradient Boost': GradientBoostingClassifier(random_state=random_state),\n",
    "        'XGBoost': XGBClassifier(),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=random_state)\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   42.6s remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.0min finished\n",
      "/Users/galopito/opt/anaconda3/envs/nf/lib/python3.6/site-packages/ipykernel_launcher.py:25: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix LogReg: \n",
      "[[10697   159  5121]\n",
      " [  584   447  1991]\n",
      " [ 2266   217 20098]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   35.8s remaining:   53.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   53.9s finished\n",
      "/Users/galopito/opt/anaconda3/envs/nf/lib/python3.6/site-packages/ipykernel_launcher.py:25: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix KNN: \n",
      "[[11915   352  3710]\n",
      " [  582  1029  1411]\n",
      " [ 2923   814 18844]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display results\n",
    "results = pd.DataFrame(columns=['Model'])\n",
    "i = 0\n",
    "for m in models.items():\n",
    "    # Building a full pipeline with our preprocessor and a Classifier\n",
    "    pipeline = Pipeline([('preprocessor', preprocessor), (m[0], m[1])])\n",
    "    # Making predictions on the training set using cross validation as well as calculating the probabilities\n",
    "    y_train_pred = cross_val_predict(pipeline,\n",
    "                                     X_train,\n",
    "                                     y_train.values.ravel(),\n",
    "                                     cv=5,\n",
    "                                     verbose=4,\n",
    "                                     n_jobs=-1)\n",
    "    # Calculating metrices\n",
    "    temp = pd.DataFrame(\n",
    "        {\n",
    "            'Accuracy': accuracy_score(y_train, y_train_pred),\n",
    "            'Recall': recall_score(y_train, y_train_pred, average=\"macro\"),\n",
    "            'Precision': precision_score(y_train, y_train_pred, average=\"macro\"),\n",
    "            'F1 Score': f1_score(y_train, y_train_pred, average=\"macro\")\n",
    "        },\n",
    "        index=[i])\n",
    "    print(f\"Confusion Matrix {m[0]}: \\n\" + str(confusion_matrix(y_train, y_train_pred)))\n",
    "    i += 1\n",
    "    results = pd.concat([results, temp])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

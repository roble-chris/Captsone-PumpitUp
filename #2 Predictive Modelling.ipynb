{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modelling : Multiclassification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Import libraries\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, average_precision_score\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, roc_curve, make_scorer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, confusion_matrix,precision_recall_curve,confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, fbeta_score, roc_auc_score\n",
    "from sklearn.metrics import plot_precision_recall_curve, plot_confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Modeling\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict,GridSearchCV,RandomizedSearchCV, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['precision_weighted']\n",
    "RSEED = 42\n",
    "pd.set_option('display.max_columns', 50)\n",
    "random_state = 100\n",
    "test_size = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of Dataset and Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of dataset after cleaning\n",
    "df = pd.read_csv('data/df_model.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Converting Labels  \n",
    "target_status_group = {'functional':2, 'functional needs repair': 1, 'non functional' : 0}\n",
    "df['status_group'] = df['status_group'].replace(target_status_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    32259\n",
       "0    22824\n",
       "1     4317\n",
       "Name: status_group, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['status_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictors and target variable\n",
    "X = df.drop([\"status_group\",'lga','recorded_by'], axis=1)\n",
    "y = df[\"status_group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41580, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59400 entries, 0 to 59399\n",
      "Data columns (total 23 columns):\n",
      "amount_tsh           59400 non-null float64\n",
      "gps_height           59400 non-null float64\n",
      "longitude            59400 non-null float64\n",
      "latitude             59400 non-null float64\n",
      "basin                59400 non-null object\n",
      "region               59400 non-null object\n",
      "lga                  59400 non-null object\n",
      "public_meeting       59400 non-null object\n",
      "recorded_by          59400 non-null object\n",
      "scheme_management    59400 non-null object\n",
      "permit               59400 non-null object\n",
      "construction_year    59400 non-null int64\n",
      "payment_type         59400 non-null object\n",
      "quality_group        59400 non-null object\n",
      "quantity             59400 non-null object\n",
      "source               59400 non-null object\n",
      "waterpoint_type      59400 non-null object\n",
      "status_group         59400 non-null int64\n",
      "funder_cat           59400 non-null object\n",
      "installer_cat        59400 non-null object\n",
      "extraction_custom    59400 non-null object\n",
      "management_custom    59400 non-null object\n",
      "population_log       59400 non-null float64\n",
      "dtypes: float64(5), int64(2), object(16)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basin',\n",
       " 'region',\n",
       " 'lga',\n",
       " 'public_meeting',\n",
       " 'recorded_by',\n",
       " 'scheme_management',\n",
       " 'permit',\n",
       " 'payment_type',\n",
       " 'quality_group',\n",
       " 'quantity',\n",
       " 'source',\n",
       " 'waterpoint_type',\n",
       " 'funder_cat',\n",
       " 'installer_cat',\n",
       " 'extraction_custom',\n",
       " 'management_custom']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating list for categorical predictors/features (used in \"Scaling with Preprocessing Pipeline\") \n",
    "cat_features = list(df.columns[df.dtypes==object])\n",
    "#cat_features.remove('status_group')\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_features.remove('permit')\n",
    "#cat_features.remove('public_meeting')\n",
    "cat_features.remove('lga')\n",
    "#cat_features.remove('ward')\n",
    "cat_features.remove('recorded_by')\n",
    "#cat_features.remove('region_code')\n",
    "#num_features.remove('num_private')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basin',\n",
       " 'region',\n",
       " 'public_meeting',\n",
       " 'scheme_management',\n",
       " 'permit',\n",
       " 'payment_type',\n",
       " 'quality_group',\n",
       " 'quantity',\n",
       " 'source',\n",
       " 'waterpoint_type',\n",
       " 'funder_cat',\n",
       " 'installer_cat',\n",
       " 'extraction_custom',\n",
       " 'management_custom']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount_tsh',\n",
       " 'gps_height',\n",
       " 'longitude',\n",
       " 'latitude',\n",
       " 'construction_year',\n",
       " 'population_log']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating list for numerical predictors/features (removing target column, used in \"Scaling with Preprocessing Pipeline\")\n",
    "num_features = list(df.columns[df.dtypes!=object])\n",
    "num_features.remove('status_group')\n",
    "#num_features.remove('district_code')\n",
    "#num_features.remove('population')\n",
    "#num_features.remove('region_code')\n",
    "num_features\n",
    "#cat_features.remove('status_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline using Pipeline\n",
    "# Pipeline for numerical features\n",
    "num_pipeline = Pipeline([\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features \n",
    "cat_pipeline = Pipeline([\n",
    "    ('1hot', OneHotEncoder(sparse=False))\n",
    "])\n",
    "\n",
    "# Complete pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (41580, 144)\n",
      "validation shape:  (17820, 144)\n"
     ]
    }
   ],
   "source": [
    "print('train shape: ', X_train_transformed.shape)\n",
    "print('validation shape: ',X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5430976430976431"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DummyClassifier for multiclass target variable \n",
    "#(2 = 'functional', 1 = 'functional needs repair', 0 = 'non functional')\n",
    "\n",
    "dummy = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "dummy.fit(X_train_transformed, y_train)\n",
    "dummy.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 54.31%\n",
      "Mean precision: 29.49%\n",
      "Mean recall: 54.31%\n"
     ]
    }
   ],
   "source": [
    "metrics = ['accuracy', 'precision_weighted', 'recall_weighted']\n",
    "scores_dummy = cross_validate(dummy, X_train_transformed, y_train, scoring=metrics, cv=5, n_jobs=-1)\n",
    "print('Mean accuracy: {:.2f}%'.format(scores_dummy['test_accuracy'].mean()*100))\n",
    "print('Mean precision: {:.2f}%'.format(scores_dummy['test_precision_weighted'].mean()*100))\n",
    "print('Mean recall: {:.2f}%'.format(scores_dummy['test_recall_weighted'].mean()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select models for comparison\n",
    "models={'Baseline': DummyClassifier(strategy='most_frequent'),\n",
    "        'LogReg': LogisticRegression(max_iter=100),\n",
    "        #'KNN': KNeighborsClassifier(),\n",
    "        #'SVC': SVC(kernel='rbf', C=1E6),\n",
    "        'Decision Tree': DecisionTreeClassifier(criterion=\"gini\", max_depth=3,random_state=random_state),\n",
    "        'Random Forest': RandomForestClassifier(random_state=random_state, max_features='sqrt', n_jobs=-1),\n",
    "        #'Gradient Boost': GradientBoostingClassifier(random_state=random_state),\n",
    "        'XGBoost': XGBClassifier(),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=random_state)\n",
    "        \n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.7s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.0s finished\n",
      "/Users/galopito/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/galopito/opt/anaconda3/envs/nf/lib/python3.6/site-packages/ipykernel_launcher.py:29: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Baseline: \n",
      "[[    0     0 15977]\n",
      " [    0     0  3022]\n",
      " [    0     0 22581]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    3.0s remaining:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    4.4s finished\n",
      "/Users/galopito/opt/anaconda3/envs/nf/lib/python3.6/site-packages/ipykernel_launcher.py:29: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix LogReg: \n",
      "[[10235   124  5618]\n",
      " [  573   293  2156]\n",
      " [ 2241   121 20219]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.0s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.3s finished\n",
      "/Users/galopito/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/galopito/opt/anaconda3/envs/nf/lib/python3.6/site-packages/ipykernel_launcher.py:29: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Decision Tree: \n",
      "[[ 8638     0  7339]\n",
      " [  683     0  2339]\n",
      " [ 2228     0 20353]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    9.9s remaining:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   12.6s finished\n",
      "/Users/galopito/opt/anaconda3/envs/nf/lib/python3.6/site-packages/ipykernel_launcher.py:29: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Random Forest: \n",
      "[[12431   270  3276]\n",
      " [  467  1034  1521]\n",
      " [ 2214   676 19691]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  1.7min remaining:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.7min finished\n",
      "/Users/galopito/opt/anaconda3/envs/nf/lib/python3.6/site-packages/ipykernel_launcher.py:29: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix XGBoost: \n",
      "[[ 9771    85  6121]\n",
      " [  382   304  2336]\n",
      " [ 1407    92 21082]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    8.5s remaining:   12.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix AdaBoost: \n",
      "[[ 9907   139  5931]\n",
      " [  553   222  2247]\n",
      " [ 2370   170 20041]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   12.7s finished\n",
      "/Users/galopito/opt/anaconda3/envs/nf/lib/python3.6/site-packages/ipykernel_launcher.py:29: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.543074</td>\n",
       "      <td>0.382262</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.294929</td>\n",
       "      <td>0.543074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.739466</td>\n",
       "      <td>0.717177</td>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.733222</td>\n",
       "      <td>0.739466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.697234</td>\n",
       "      <td>0.661340</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.655454</td>\n",
       "      <td>0.697234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.797403</td>\n",
       "      <td>0.791715</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.790723</td>\n",
       "      <td>0.797403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.749327</td>\n",
       "      <td>0.724636</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.758308</td>\n",
       "      <td>0.749327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.725589</td>\n",
       "      <td>0.701868</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.712780</td>\n",
       "      <td>0.725589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  F1 Score          Model  Precision    Recall\n",
       "0  0.543074  0.382262       Baseline   0.294929  0.543074\n",
       "1  0.739466  0.717177         LogReg   0.733222  0.739466\n",
       "2  0.697234  0.661340  Decision Tree   0.655454  0.697234\n",
       "3  0.797403  0.791715  Random Forest   0.790723  0.797403\n",
       "4  0.749327  0.724636        XGBoost   0.758308  0.749327\n",
       "5  0.725589  0.701868       AdaBoost   0.712780  0.725589"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scores = cross_validate(models, X_train, y_train, scoring=scoring, cv=3, n_jobs=-1, verbose=10)\n",
    "# Calculate and display results\n",
    "results = pd.DataFrame(columns=['Model'])\n",
    "i = 0\n",
    "for m in models.items():\n",
    "       \n",
    "    # Building a full pipeline with our preprocessor and a Classifier\n",
    "    pipeline = Pipeline([('preprocessor', preprocessor), (m[0], m[1])])\n",
    "    # Making predictions on the training set using cross validation as well as calculating the probabilities\n",
    "    y_train_pred = cross_val_predict(pipeline,\n",
    "                                        X_train,\n",
    "                                        y_train.values.ravel(),\n",
    "                                        cv=5,\n",
    "                                        verbose=4,\n",
    "                                        n_jobs=-1)\n",
    "    # Calculating metrices\n",
    "    temp = pd.DataFrame(\n",
    "        {\n",
    "            'Model': m[0],\n",
    "            'Accuracy': accuracy_score(y_train, y_train_pred),\n",
    "            'Recall': recall_score(y_train, y_train_pred, average=\"weighted\"),\n",
    "            'Precision': precision_score(y_train, y_train_pred, average=\"weighted\"),\n",
    "            'F1 Score': f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "            \n",
    "        },\n",
    "        index=[i])\n",
    "    print(f\"Confusion Matrix {m[0]}: \\n\" + str(confusion_matrix(y_train, y_train_pred)))\n",
    "    i += 1\n",
    "    results = pd.concat([results, temp])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression (using pipeline)\n",
    "pipeline_logreg = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(pipeline_logreg,X_train,y_train,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores:\n",
      "-------------------------\n",
      "Accuracy: 0.74\n",
      "Recall: 0.74\n",
      "Precision: 0.73\n",
      "F1 Score: 0.72\n",
      "Confusion Matrix: \n",
      "[[10262   119  5596]\n",
      " [  580   292  2150]\n",
      " [ 2251   121 20209]]\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression (using pipeline, printing results)\n",
    "print('Cross validation scores:')\n",
    "print('-------------------------')\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_train, y_train_pred)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_train, y_train_pred,average='weighted')))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_train, y_train_pred,average='weighted')))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_train, y_train_pred,average='weighted')))\n",
    "print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y_train, y_train_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunig with Gread Seach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters for grid-search (C initial: [0.01, 0.1, 1, 10, 100]; adapted according to optimal results)\n",
    "param_logreg = {'logreg__penalty':('l1','l2'),\n",
    "                'logreg__C': [0.05, 0.08, 0.1, 0.2, 0.5, 1],\n",
    "                'logreg__class_weight': [{0: x, 1: 1.0-x} for x in [0.25,0.5,0.75]]\n",
    "               }\n",
    "\n",
    "grid_logreg = GridSearchCV(pipeline_logreg, param_grid=param_logreg, cv=5, scoring=scoring, \n",
    "                           verbose=5, n_jobs=-1, refit='precision_weighted',return_train_score=True) # scoring can also be \"precision\", \"recall\", ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        ColumnTransformer(transformers=[('num',\n",
       "                                                                         Pipeline(steps=[('std_scaler',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         ['amount_tsh',\n",
       "                                                                          'gps_height',\n",
       "                                                                          'longitude',\n",
       "                                                                          'latitude',\n",
       "                                                                          'construction_year',\n",
       "                                                                          'population_log']),\n",
       "                                                                        ('cat',\n",
       "                                                                         Pipeline(steps=[('1hot',\n",
       "                                                                                          OneHotEncoder(sparse=False))]),\n",
       "                                                                         ['basin',\n",
       "                                                                          'region',\n",
       "                                                                          'public_meeting',\n",
       "                                                                          'scheme_managem...\n",
       "                                                                          'installer_cat',\n",
       "                                                                          'extraction_custom',\n",
       "                                                                          'management_custom'])])),\n",
       "                                       ('logreg',\n",
       "                                        LogisticRegression(max_iter=1000))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'logreg__C': [0.05, 0.08, 0.1, 0.2, 0.5, 1],\n",
       "                         'logreg__class_weight': [{0: 0.25, 1: 0.75},\n",
       "                                                  {0: 0.5, 1: 0.5},\n",
       "                                                  {0: 0.75, 1: 0.25}],\n",
       "                         'logreg__penalty': ('l1', 'l2')},\n",
       "             refit='precision_weighted', return_train_score=True,\n",
       "             scoring=['precision_weighted'], verbose=5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "grid_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.76\n",
      "Best parameters:\n",
      "{'logreg__C': 0.5, 'logreg__class_weight': {0: 0.75, 1: 0.25}, 'logreg__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Show best parameters\n",
    "print('Best score:\\n{:.2f}'.format(grid_logreg.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_logreg.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model as best_model\n",
    "best_model_logreg = grid_logreg.best_estimator_['logreg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n",
      "Recall: 0.73\n",
      "Precision: 0.75\n",
      "F1 Score: 0.70\n",
      "Confusion Matrix: \n",
      "[[4052    0 2795]\n",
      " [ 186    4 1105]\n",
      " [ 668    1 9009]]\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy, recall and precision for the test set with the optimized model\n",
    "y_pred_logreg = best_model_logreg.predict(X_test_transformed)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred_logreg)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_test, y_pred_logreg,average='weighted')))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_test, y_pred_logreg,average='weighted')))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_test, y_pred_logreg,average='weighted')))\n",
    "print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y_test, y_pred_logreg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Support Vector Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "pipeline_rf_clf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('rf_clf', RandomForestClassifier(n_estimators=100,\n",
    "                              random_state=random_state,\n",
    "                              max_depth=5,\n",
    "                              max_features=\"sqrt\",\n",
    "                              n_jobs=-1,\n",
    "                              class_weight='balanced',\n",
    "                              criterion= 'entropy',\n",
    "                              min_samples_split= 10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_rf_clf = cross_val_predict(pipeline_rf_clf, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores:\n",
      "-------------------------\n",
      "Accuracy: 0.64\n",
      "Recall: 0.64\n",
      "Precision: 0.73\n",
      "F1 Score: 0.67\n",
      "Confusion Matrix: \n",
      "[[ 9268  2382  4327]\n",
      " [  347  1730   945]\n",
      " [ 1961  4835 15785]]\n"
     ]
    }
   ],
   "source": [
    "print('Cross validation scores:')\n",
    "print('-------------------------')\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_train, y_train_pred_rf_clf)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_train, y_train_pred_rf_clf,average='weighted')))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_train, y_train_pred_rf_clf,average='weighted')))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_train, y_train_pred_rf_clf,average='weighted')))\n",
    "print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y_train, y_train_pred_rf_clf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameter space for grid-search\n",
    "param_grid = {'rf_clf__bootstrap': [True, False],\n",
    "              'rf_clf__max_depth': [2, 3, 5, 10, 20, 30,50,None],\n",
    "              'rf_clf__max_features': ['auto', 'sqrt',3,5,10,20],\n",
    "              'rf_clf__min_samples_leaf': [1, 2, 4],\n",
    "              'rf_clf__min_samples_split': [2, 5, 10],\n",
    "              'rf_clf__n_estimators': [10, 50, 100, 200, 400]} # Others: kernel, degree (only for poly)\n",
    "grid_rf_clf = RandomizedSearchCV(pipeline_rf_clf, param_grid, cv=5, scoring=scoring, \n",
    "                           verbose=10, n_jobs=-1, refit='precision_weighted',return_train_score=True, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 15.3min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 17.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('preprocessor',\n",
       "                                              ColumnTransformer(transformers=[('num',\n",
       "                                                                               Pipeline(steps=[('std_scaler',\n",
       "                                                                                                StandardScaler())]),\n",
       "                                                                               ['amount_tsh',\n",
       "                                                                                'gps_height',\n",
       "                                                                                'longitude',\n",
       "                                                                                'latitude',\n",
       "                                                                                'construction_year',\n",
       "                                                                                'population_log']),\n",
       "                                                                              ('cat',\n",
       "                                                                               Pipeline(steps=[('1hot',\n",
       "                                                                                                OneHotEncoder(sparse=False))]),\n",
       "                                                                               ['basin',\n",
       "                                                                                'region',\n",
       "                                                                                'public_meeting',\n",
       "                                                                                'scheme_m...\n",
       "                   param_distributions={'rf_clf__bootstrap': [True, False],\n",
       "                                        'rf_clf__max_depth': [2, 3, 5, 10, 20,\n",
       "                                                              30, 50, None],\n",
       "                                        'rf_clf__max_features': ['auto', 'sqrt',\n",
       "                                                                 3, 5, 10, 20],\n",
       "                                        'rf_clf__min_samples_leaf': [1, 2, 4],\n",
       "                                        'rf_clf__min_samples_split': [2, 5, 10],\n",
       "                                        'rf_clf__n_estimators': [10, 50, 100,\n",
       "                                                                 200, 400]},\n",
       "                   refit='precision_weighted', return_train_score=True,\n",
       "                   scoring=['precision_weighted'], verbose=10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit on training data\n",
    "grid_rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.80\n",
      "Best parameters:\n",
      "{'rf_clf__n_estimators': 400, 'rf_clf__min_samples_split': 5, 'rf_clf__min_samples_leaf': 2, 'rf_clf__max_features': 20, 'rf_clf__max_depth': 20, 'rf_clf__bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "# Show best parameters\n",
    "print('Best score:\\n{:.2f}'.format(grid_rf_clf.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid_rf_clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model as best_model\n",
    "best_model_rf_clf = grid_rf_clf.best_estimator_['rf_clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n",
      "Recall: 0.73\n",
      "Precision: 0.68\n",
      "F1 Score: 0.69\n",
      "Confusion Matrix: \n",
      "[[5226  401 1220]\n",
      " [ 141  790  364]\n",
      " [ 788 1062 7828]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_rf_clf = best_model_rf_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred_rf_clf)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_test, y_pred_rf_clf,average='macro')))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_test, y_pred_rf_clf,average='macro')))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_test, y_pred_rf_clf,average='macro')))\n",
    "print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y_test, y_pred_rf_clf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pipeline to use in RandomSearchCV and GridSearchCV\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb_reg', XGBClassifier(n_estimators=110,\n",
    "                              random_state=random_state,\n",
    "                              max_depth=5,\n",
    "                              max_features=20,\n",
    "                              scoring=scoring,\n",
    "                              n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## : SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "seed=100\n",
    "sm = SMOTE(sampling_strategy='minority',random_state=seed)\n",
    "X_res, y_res = sm.fit_resample(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_new, y_train_new = sm.fit_sample(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1c164668>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# observe that data has been balanced\n",
    "pd.Series(y_train_new).value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/galopito/opt/anaconda3/envs/nf/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "clf.fit(X_train_new, y_train_new)\n",
    "\n",
    "# prediction for Training data\n",
    "train_pred_sm = clf.predict(X_train_new)\n",
    "\n",
    "# prediction for Testing data\n",
    "test_pred_sm = clf.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Training Dataset =  0.6680367396106205\n",
      "Accuracy score for Testing Dataset =  0.6404865508806377\n",
      "Accuracy: 0.66\n",
      "Recall: 0.65\n",
      "Precision: 0.68\n",
      "F1 Score: 0.66\n",
      "Confusion Matrix: \n",
      "[[ 9161  2952  3864]\n",
      " [ 1371 16319  4891]\n",
      " [ 1549  6184 14848]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Accuracy score for Training Dataset = ', precision_score(train_pred_sm, y_train_new, average='weighted'))\n",
    "print('Accuracy score for Testing Dataset = ', precision_score(test_pred_sm, y_test,average='weighted'))\n",
    "\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_train_new, train_pred_sm)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_train_new, train_pred_sm,average='macro')))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_train_new, train_pred_sm,average='macro')))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score(y_train_new, train_pred_sm,average='macro')))\n",
    "print(\"Confusion Matrix: \\n\" + str(confusion_matrix(y_train_new, train_pred_sm)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
